{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d210303-a20f-4ffb-8ddf-6245d984238b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e5781d-1b5a-405e-b237-a318ddcfbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm.auto as tqdm\n",
    "import torch\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771fbfed-11fa-4d1f-bfef-59b3ad78f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(x: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert a torch Tensor to numpy array\"\"\"\n",
    "    return x.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f448a74-0708-45c9-95c9-05e65f276356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(x):\n",
    "    \"\"\"Wrap angle into range [-pi, pi]\"\"\"\n",
    "    return (x + np.pi) % (2*np.pi) - np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90135ffb-6974-4846-bf0e-03952e128a99",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Brief ML primer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ec273-fb6b-43ac-b2a3-d2b4feb8f889",
   "metadata": {},
   "source": [
    "Let's quickly demonstrate how training looks in Pytorch. We will train a small neural network to model the function\n",
    "$$\n",
    "f(x) = \\mathrm{sinc}(x) := \\frac{\\sin(\\pi x)}{\\pi x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a415a-49a6-4b19-ae48-6717574e7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_fn(x):\n",
    "    return torch.sinc(x)\n",
    "fig, ax = plt.subplots(1,1, figsize=(3.5, 2.5))\n",
    "xs = torch.linspace(-5, 5, steps=51)\n",
    "ys = target_fn(xs)\n",
    "ax.plot(grab(xs), grab(ys))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f68df1-8cb3-4f4c-8d8f-0750a8c6e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 8),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(8, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 1, 'x should just have a batch index'\n",
    "        return self.net(x[:,None])[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e7f54-7d3a-42da-846b-8cd95d3afa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = ToyModel()\n",
    "    batch_size = 128\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_hist = []\n",
    "    for i in tqdm.tqdm(range(25000)):\n",
    "        opt.zero_grad()\n",
    "        # random samples around 0 for the training points\n",
    "        x = 3*torch.randn((batch_size,))\n",
    "        model_y = model(x)\n",
    "        true_y = target_fn(x)\n",
    "        # mean squared error\n",
    "        loss = ((true_y - model_y)**2).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_hist.append(grab(loss))\n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(f'Step {i+1}: Loss {grab(loss)}')\n",
    "    return dict(model=model, loss=np.stack(loss_hist))\n",
    "res = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c1c1d-f59a-4d5e-907c-30d583170e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(8, 3), tight_layout=True)\n",
    "xs = torch.linspace(-5, 5, steps=51)\n",
    "true_ys = target_fn(xs)\n",
    "model_ys = res['model'](xs)\n",
    "ax = axes[0]\n",
    "ax.plot(grab(xs), grab(true_ys), color='k', label='target')\n",
    "ax.plot(grab(xs), grab(model_ys), color='xkcd:red', label='model')\n",
    "ax.legend()\n",
    "ax = axes[1]\n",
    "ax.plot(res['loss'])\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948c24e-ea95-483c-8b69-0a24b16af98a",
   "metadata": {},
   "source": [
    "# Action\n",
    "The general form of the action is\n",
    "$S(\\theta_1, \\theta_2; \\alpha, \\beta) := -\\beta \\cos(\\theta_1 - \\theta_2) - \\alpha \\cos(\\theta_1) + \\alpha \\cos(\\theta_2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26226ba9-07cd-40e4-9cda-8c1b3ccb1dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action(th, *, alpha, beta):\n",
    "    \"\"\"family of actions on two angles\"\"\"\n",
    "    assert th.shape[-1] == 2\n",
    "    th1, th2 = th[...,0] ,th[...,1]\n",
    "    return (\n",
    "        -beta * torch.cos(th1 - th2) - alpha * torch.cos(th1)\n",
    "        + alpha * torch.cos(th2)\n",
    "    )\n",
    "\n",
    "def make_action(alpha, beta):\n",
    "    return lambda th: action(th, alpha=alpha, beta=beta)\n",
    "\n",
    "# some target parameters\n",
    "beta_target = 3.0\n",
    "alpha_target = 1.0\n",
    "target_action = make_action(alpha_target, beta_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e9cac-0f6f-4d2b-929c-60edb0cccd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_inds(weights):\n",
    "    \"\"\"resample indices according to weights\"\"\"\n",
    "    p = np.copy(weights)\n",
    "    p /= np.sum(p)\n",
    "    return np.random.choice(len(weights), p=p, size=len(weights))\n",
    "\n",
    "def sample(batch_size, action, *, beta0):\n",
    "    \"\"\"importance sampling to get ground truth data\"\"\"\n",
    "    shape = (batch_size,)\n",
    "    dist = torch.distributions.VonMises(0.0, beta0)\n",
    "    delta = dist.sample(shape)\n",
    "    S0 = dist.log_prob(delta)\n",
    "    th1 = 2*np.pi*torch.rand(size=shape)\n",
    "    th2 = (th1 - delta) % (2*np.pi)\n",
    "    th = torch.stack([th1, th2], axis=-1)\n",
    "    logw = -action(th) + S0\n",
    "    logw -= torch.logsumexp(logw, dim=0)\n",
    "    weight = np.exp(grab(logw))\n",
    "    # resample\n",
    "    inds = sample_inds(weight)\n",
    "    return th[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c9986e-173b-42dd-a27f-7ec81aa244f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_th_grid(steps):\n",
    "    th = torch.linspace(-np.pi, np.pi, steps=steps)\n",
    "    th = (th[1:]+th[:-1])/2\n",
    "    th = torch.stack(torch.meshgrid([th, th], indexing='ij'), axis=-1)\n",
    "    return th\n",
    "def plot_dist(action, *, ax, nsteps=60):\n",
    "    th = make_th_grid(nsteps)\n",
    "    S = action(th)\n",
    "    th = grab(th)\n",
    "    ax.contourf(th[...,0], th[...,1], np.exp(-grab(S)))\n",
    "def plot_samples(th, *, ax, nbins=60):\n",
    "    bins = np.linspace(-np.pi, np.pi, num=nbins+1)\n",
    "    th = wrap(grab(th))\n",
    "    ax.hist2d(th[...,0], th[...,1], bins=bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65c178-cda9-43b1-8afd-d945a842c57e",
   "metadata": {},
   "source": [
    "# Normalizing flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182a873-c849-4321-b635-242707bb9d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelVelocity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2+2+1, 32),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(32, 2),\n",
    "        )\n",
    "    def value(self, th, t):\n",
    "        assert th.shape[-1] == 2\n",
    "        t_expand = (t * torch.ones(th.shape[:-1])).unsqueeze(-1)\n",
    "        return self.net(torch.cat([torch.cos(th), torch.sin(th), t_expand], dim=-1))\n",
    "    def div(self, th, t):\n",
    "        # NOTE: for high dimensions, this is expensive!\n",
    "        # vmap allows th to have a batch index over which we vectorize this operation\n",
    "        J = torch.func.vmap(torch.func.jacfwd(self.value, argnums=0))(th, t)\n",
    "        trJ = torch.einsum('...ii->...', J)\n",
    "        return trJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b1207-b7b0-4364-9916-6ca8e8a39efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_flow(th, model, *, nsteps, tf=1.0, reverse=False):\n",
    "    dt = tf/nsteps\n",
    "    logJ = 0\n",
    "    steps = range(nsteps)\n",
    "    if reverse:\n",
    "        steps = reversed(steps)\n",
    "    for i in steps:\n",
    "        t = i*dt\n",
    "        t = torch.ones((th.shape[0],)) * t\n",
    "        v = model.value(th, t)\n",
    "        assert v.shape == th.shape\n",
    "        div = model.div(th, t)\n",
    "        sign = -1 if reverse else 1\n",
    "        th = wrap(th + sign * dt * v)\n",
    "        logJ = logJ + div * dt\n",
    "    return th, logJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6756ed-9ff6-4df0-933f-f9f23b3a8332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_from_flow(model, *, nsteps, tf=1.0):\n",
    "    def action(th):\n",
    "        _, logJ = apply_flow(th, model, nsteps=nsteps, tf=tf, reverse=True)\n",
    "        return logJ - 2*np.log(2*np.pi)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3b31b-6e13-4f9d-bc92-d1ff4cbecdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ess(logw):\n",
    "    \"\"\"effective sample size = <w>^2 / <w^2>\"\"\"\n",
    "    return torch.exp(2*torch.logsumexp(logw, dim=0) - torch.logsumexp(2*logw, dim=0)) / len(logw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110970e-f213-4613-88ee-90b1616adf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, *, n_train, batch_size):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    hist = dict(loss=[], ess=[])\n",
    "    for _ in tqdm.tqdm(range(n_train)):\n",
    "        optimizer.zero_grad()\n",
    "        prior_th = wrap(2*np.pi*torch.rand((batch_size, 2)))\n",
    "        logr = -2*np.log(2*np.pi)\n",
    "        flow_th, _ = apply_flow(prior_th, model, nsteps=50)\n",
    "        # path gradients evaluation of logq\n",
    "        model.requires_grad_(False)\n",
    "        _, logJ = apply_flow(flow_th, model, nsteps=50, reverse=True)\n",
    "        model.requires_grad_(True)\n",
    "        logq = logr - logJ\n",
    "        logp = -target_action(flow_th)\n",
    "        loss = (logq - logp).mean()\n",
    "        hist['loss'].append(grab(loss))\n",
    "        with torch.no_grad():\n",
    "            ess = compute_ess(logp - logq)\n",
    "        hist['ess'].append(grab(ess))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af20ac-f13e-4242-b309-11b40ae96fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelVelocity()\n",
    "hist = train(model, n_train=200, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369b520-ce57-4a6c-863f-546552829a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,1)\n",
    "axes[0].plot(hist['loss'])\n",
    "axes[0].set_ylabel('loss')\n",
    "axes[1].plot(hist['ess'])\n",
    "axes[1].set_ylabel('ess')\n",
    "axes[1].set_xlabel('train iter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de9c3f-97ea-4007-a5ca-9fc2521d2f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "prior_th = wrap(2*np.pi*torch.rand((16000, 2)))\n",
    "with torch.no_grad():\n",
    "    flow_th = apply_flow(prior_th, model, nsteps=100)[0]\n",
    "plot_samples(flow_th, ax=ax)\n",
    "ax.set_aspect(1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da81a35f-4501-4bba-9695-1ad0eda234c1",
   "metadata": {},
   "source": [
    "**EXERCISE:** Adjust the model and training parameters to optimize the final loss and ESS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4a91d9-8a7a-4758-a28c-72bf27e986d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_coeffs_grid(S):\n",
    "    \"\"\"extract Wilson-like coeffs using the Fourier transform\"\"\"\n",
    "    Sk = np.fft.ifft2(S)\n",
    "    c = Sk[0,0]\n",
    "    a1 = Sk[0,1] + Sk[0,-1]\n",
    "    a2 = Sk[1,0] + Sk[-1,0]\n",
    "    b1 = Sk[1,1] + Sk[-1,-1]\n",
    "    b2 = Sk[1,-1] + Sk[-1,1]\n",
    "    return dict(c=c, a1=a1, a2=a2, b1=b1, b2=b2)\n",
    "def measure_coeffs(action):\n",
    "    th = make_th_grid(50)\n",
    "    grid = th.shape[:-1]\n",
    "    S = grab(action(th.flatten(0,1)).reshape(grid))\n",
    "    return measure_coeffs_grid(S)\n",
    "def plot_coeffs(ts, coeffs, x='a1', y='b2', *, ax, cmap, marker='.', label=None):\n",
    "    pts = np.stack([(coeff[x], coeff[y]) for coeff in coeffs], axis=1)\n",
    "    cmap = plt.get_cmap(cmap)\n",
    "    ax.scatter(*pts, marker=marker, s=3, color=cmap(ts), label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a5b25-25be-4082-9a40-b5dcce1f055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_coeffs = measure_coeffs(target_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfe0b4-dd63-464c-8892-306901a24745",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.linspace(0.0, 1.0, num=11)\n",
    "with torch.no_grad():\n",
    "    flow_coeffs = [\n",
    "        measure_coeffs(action_from_flow(model, nsteps=100, tf=t))\n",
    "        for t in ts\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28297b1-2bd2-4d81-9267-1cf089493c03",
   "metadata": {},
   "source": [
    "We can finally look at the path through distribution space learned by the continuous normalizing flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c5326-9d03-4027-a63d-7dd7e03f7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(3, 3), tight_layout=True)\n",
    "plot_coeffs(ts, flow_coeffs, ax=ax, cmap='Reds_r', marker='o', label='Normalizing flow')\n",
    "plot_coeffs([1.0], [target_coeffs], ax=ax, cmap='Greys', marker='x', label='Target')\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel(r'$\\beta$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
